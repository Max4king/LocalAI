# version: '3.6'

services:
  api:
    # See https://localai.io/basics/getting_started/#container-images for
    # a list of available container images (or build your own with the provided Dockerfile)
    # Available images with CUDA, ROCm, SYCL
    # Image list (quay.io): https://quay.io/repository/go-skynet/local-ai?tab=tags
    # Image list (dockerhub): https://hub.docker.com/r/localai/localai
    # image: localai/localai:latest-aio-gpu-nvidia-cuda-12
    image: localai/localai:v2.14.0-cublas-cuda12-core
    build:
      context: .
      dockerfile: Dockerfile
      args:
      - IMAGE_TYPE=core
      - BASE_IMAGE=ubuntu:22.04
    ports:
      - 8080:8080
    env_file:
      - .env
    environment:
      - MODELS_PATH=/models
      # - DEBUG=true
    volumes:
      - ./models:/models:cached
      - ./images/:/tmp/generated/images/
    # command:
    #   - mixtral-instruct
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              # device_ids: ['1']
              capabilities: [gpu]
